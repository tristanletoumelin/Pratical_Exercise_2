{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning in Path Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional shortest path algorithm including BFS(Breadth First Search), DFS(Depth First Search) and Dijkstra's algorithm.\n",
    "\n",
    "However, BFS and DFS is very slow and suffer exponential time increase as graph tree become deeper and deeper for large complex graph. For my experience, when I have 70k nodes and 200K edge graph, BFS and DFS would take hours to search 5 depth. So it is impossible if the shortest path is 6 depth or more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='p1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='P2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Dijkstra's algorithm, it is quick to give a shortest path, but it will give only one not all of the best path. For instance, if there are 100 equal best rountes in a weighted graph, Dijskra's algorithm will only return one of the best rountes. \n",
    "\n",
    "So I develop a algorithm using reinforcemenet learning in path finding problem. In reinforcemnt learning problem, we have action, rewards and states and discount rate. To solve traditional problem, we have Q-learning. Q-learning can be used to find an optimal action-selection policy for any given (finite) Markov decision process (MDP). Similarly, to solve best-path-finding problem, we have Q-rounting. The difference between Q-Learning and Q-Rounting is that, Q-Rounting doesn't have a discount rate, and for each state, it will choose the minimun furture cost instead of maximum future reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='p3.png'>\n",
    "<img src='p4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python code for Q-Rounting:\n",
    "```python\n",
    "def update_Q(T,Q,current_state, next_state, alpha):\n",
    "    current_t = T[current_state][next_state]\n",
    "    current_q = Q[current_state][next_state]\n",
    "    new_q = current_q + alpha * (current_t + min(Q[next_state].values()) - current_q)\n",
    "    Q[current_state][next_state] = new_q   \n",
    "    return Q\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we take a small network for instance.\n",
    "<img src='p5.png'>\n",
    "We use number 1 to 9  to repalce node A to I. And we want to find shortest path from 1(A) to 9(I)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop: 0\n",
      "nodes: [0, 0, 6]\n",
      "nodes: [0, 0, 6, 4]\n",
      "nodes: [0, 0, 6, 4, 4]\n",
      "nodes: [0, 0, 6, 4, 4, 4]\n",
      "time is: 0.008524894714355469\n",
      "{9: [[1, 4, 5, 9]]}\n"
     ]
    }
   ],
   "source": [
    "from get_dict import get_dict\n",
    "from get_R_Q import initial_R\n",
    "from get_R_Q import initial_Q\n",
    "from get_result import get_result\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "data = pd.read_csv(\"graph_1.csv\")\n",
    "graph = get_dict(data)\n",
    "\n",
    "A = graph[\"A\"]\n",
    "Z = graph[\"Z\"]\n",
    "weight = graph[\"weight\"]\n",
    "A_Z_dict = graph[\"A_Z_dict\"]\n",
    "\n",
    "##\n",
    "start = 1\n",
    "end = [9]\n",
    "\n",
    "R = initial_R(A,Z,weight,A_Z_dict)\n",
    "Q = initial_Q(R)\n",
    "\n",
    "alpha = 0.7 # learning rate\n",
    "epsilon = 0.1 #greedy policy\n",
    "n_episodes = 1000\n",
    "\n",
    "time0 = time.time()\n",
    "result = get_result(R,Q,alpha,epsilon,n_episodes,start,end)\n",
    "print(\"time is:\",time.time() - time0)\n",
    "print(result[\"all_routes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see it find the shortest is 1-4-5-9(A-D-E-I) and the cost for this rounte is 23.\n",
    "Now we change the weight from 4(D) to 9(I) to 18. Now we expect it will finde another rount 1-4-9(A-D-I), since both route have cost 23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop: 0\n",
      "nodes: [0, 0, 8]\n",
      "nodes: [0, 0, 8, 4]\n",
      "nodes: [0, 0, 8, 4, 4]\n",
      "nodes: [0, 0, 8, 4, 4, 4]\n",
      "time is: 0.006042003631591797\n",
      "{9: [[1, 4, 9], [1, 4, 5, 9]]}\n"
     ]
    }
   ],
   "source": [
    "from get_dict import get_dict\n",
    "from get_R_Q import initial_R\n",
    "from get_R_Q import initial_Q\n",
    "from get_result import get_result\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "data = pd.read_csv(\"graph_2.csv\")\n",
    "graph = get_dict(data)\n",
    "\n",
    "A = graph[\"A\"]\n",
    "Z = graph[\"Z\"]\n",
    "weight = graph[\"weight\"]\n",
    "A_Z_dict = graph[\"A_Z_dict\"]\n",
    "\n",
    "##\n",
    "start = 1\n",
    "end = [9]\n",
    "\n",
    "R = initial_R(A,Z,weight,A_Z_dict)\n",
    "Q = initial_Q(R)\n",
    "\n",
    "alpha = 0.7 # learning rate\n",
    "epsilon = 0.1 #greedy policy\n",
    "n_episodes = 1000\n",
    "\n",
    "time0 = time.time()\n",
    "result = get_result(R,Q,alpha,epsilon,n_episodes,start,end)\n",
    "print(\"time is:\",time.time() - time0)\n",
    "print(result[\"all_routes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see it find two routes have same smallest cost 23.\n",
    "\n",
    "This is only a example of small network. For my experience, I tried on large complex network with 70k nodes and 20k edges and Q-routing algorithm is still fast and it can find all shortest path with 10 depth within minutes. \n",
    "\n",
    "You set your own graph in graph.csv. First column is stating node, second column is connecting node and third column is the cost between the two nodes.\n",
    "\n",
    "Hope you have fun!! If you have any questions, please leave comments or e-mail to mathshiluyuan@gmail.com."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
